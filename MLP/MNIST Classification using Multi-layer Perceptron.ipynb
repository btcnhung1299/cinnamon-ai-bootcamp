{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input layer\n",
    "- Number of hidden layers: $1$ with $n$ units\n",
    "    + Activation function: sigmoid\n",
    "- Output layer: $10$ units\n",
    "    + Activation function: softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the one and only hidden layer, we use <b>sigmoid function</b> as the activation function. Sigmoid function is defined by the formula:\n",
    "\n",
    "$$f(z_i) = \\frac{1}{1 + e^{-z_i}}$$\n",
    "\n",
    "where $z_i$ is a <b>linear transformation</b> for class $i (1 \\leq i \\leq D^f)$\n",
    "\n",
    "!!!! $$\\mathbf{Z} = \\mathbf{X} \\mathbf{W^T} + \\mathbf{b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(X, W, b):\n",
    "    Z = np.dot(X, W.T) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1. / (1 + np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the probabilities for each class in output layer, <b>softmax function</b> is applied as\n",
    "\n",
    "$$f(z_i) = \\frac{e^{z_i}}{\\sum_{i = 1}^{D^f}e^{z_i}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    A = np.exp(Z)\n",
    "    A = A / np.sum(A, axis=1, keepdims=True)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random assignment is utilized to initialize weight matrices with small values ranging from -0.5 to 0.5 ($\\alpha = 0.5$) while we use bias = 0 for all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(num_layers, layer_dims, alpha=0.5, random_state=None):\n",
    "    np.random.seed(random_state)\n",
    "    W = [None] * num_layers\n",
    "    b = [None] * num_layers\n",
    "    diff_W = [None] * num_layers\n",
    "    diff_b = [None] * num_layers\n",
    "    \n",
    "    for i in range(1, num_layers):\n",
    "        W[i] = np.random.rand(layer_dims[i], layer_dims[i - 1]) - alpha\n",
    "        b[i] = np.zeros((1, layer_dims[i]))\n",
    "        diff_W[i] = np.zeros((layer_dims[i], layer_dims[i - 1]))\n",
    "        diff_b[i] = np.zeros((1, layer_dims[i]))\n",
    "        \n",
    "    return W, diff_W, b, diff_b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(num_layers, X, W, b):\n",
    "    A = [None] * num_layers\n",
    "    Z = [None] * num_layers\n",
    "    A[0] = X\n",
    "    \n",
    "    # Input and hidden layers\n",
    "    i = 1\n",
    "    while i < num_layers - 1:\n",
    "        Z[i] = linear_forward(A[i - 1], W[i], b[i])\n",
    "        A[i] = sigmoid(Z[i])\n",
    "        i += 1\n",
    "    \n",
    "    # Output layer\n",
    "    Z[i] = linear_forward(A[i - 1], W[i], b[i])\n",
    "    A[i] = softmax(Z[i])\n",
    "    \n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{t}$ be a vector of target labels and $\\mathbf{y}$ be a vector of predicted label of the current model. Dissimilarity between $\\mathbf{t}$ and $\\mathbf{y}$ is measured by <b>cross entropy</b>, which is\n",
    "\n",
    "$$\\mathbf{L}(\\mathbf{y}, \\mathbf{t}) = -\\sum_{i = 1}^{D^f}{t_i \\log y_i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(t, y):\n",
    "    cost = np.sum(np.multiply(np.log(y), t) + np.multiply(np.log(1 - y), t), axis=1)\n",
    "    return -np.mean(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize weight $\\theta$ used in the current model such that minimizes the loss function, which is the ultimate goal of learning for neural network, we use <b>gradient descent</b>. The algorithm works by repeated iterative throught two steps:\n",
    "\n",
    "- Compute $\\theta_{new} = \\theta - \\eta \\nabla_{\\theta} \\mathbf{L} + \\gamma \\times (\\theta - \\theta_{old})$\n",
    "\n",
    "- Update $\\theta$ and get new $\\mathbf{y}$\n",
    "\n",
    "\n",
    "Derivatives are utilized in gradient descent in order to detect which one causes the most error and change parameters in that direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(y, A):\n",
    "    return (A - y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Derivative of sigmoid function:\n",
    "\n",
    "$$\\frac{\\delta f(z_i)}{\\delta z_i} = \\delta z_i \\times f(z_i) \\times (1 - f(z_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(dA, A):\n",
    "    dZ = dA * A * (1 - A)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Derivative of weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(dZ, A):\n",
    "    dW = np.dot(dZ.T, A)\n",
    "    db = np.sum(dZ, axis=0, keepdims=True)\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reverse Linear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, W):\n",
    "    return np.dot(dZ, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(num_layers, learning_rate, momentum_factor, W, dW, diff_W, b, db, diff_b):\n",
    "    for i in range(1, num_layers):\n",
    "        new_W = W[i] - learning_rate * dW[i] + momentum_factor * diff_W[i]\n",
    "        diff_W[i] = new_W - W[i]\n",
    "        W[i] = new_W\n",
    "        \n",
    "        new_b = b[i] - learning_rate * db[i] + momentum_factor * diff_b[i]\n",
    "        diff_b[i] = new_b - b[i]\n",
    "        b[i] = new_b\n",
    "        \n",
    "    return W, diff_W, b, diff_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(num_layers, y, W, A, Z):\n",
    "    dA = [None] * num_layers\n",
    "    dZ = [None] * num_layers\n",
    "    dW = [None] * num_layers\n",
    "    db = [None] * num_layers\n",
    "    \n",
    "    # Gradient descent on output layer\n",
    "    i = num_layers - 1\n",
    "    dZ[i] = error(y, A[i])\n",
    "    dW[i], db[i] = gradient(dZ[i], A[i - 1])\n",
    "    i -= 1\n",
    "    \n",
    "    # Gradient descent on hidden layers\n",
    "    while i > 0:\n",
    "        dA[i] = linear_backward(dZ[i + 1], W[i + 1])\n",
    "        dZ[i] = sigmoid_gradient(dA[i], A[i])\n",
    "        dW[i], db[i] = gradient(dZ[i], A[i - 1])\n",
    "        i -= 1\n",
    "        \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_curve(costs):\n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_mlp(X, y, num_units,\n",
    "               learning_rate=0.01, momentum_factor=0.9,\n",
    "               num_iterations=1, plot_cost=True, random_state=None):\n",
    "    \n",
    "    input_dim, num_classes = X.shape[1], y.shape[1]\n",
    "    layer_dims = [input_dim, num_units, num_classes]\n",
    "    num_layers = len(layer_dims)\n",
    "\n",
    "    W, diff_W, b, diff_b = init_params(num_layers, layer_dims, 0.5, random_state)\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A, Z = forward_propagation(num_layers, X, W, b)\n",
    "        cost = compute_cost(y, A[num_layers - 1])\n",
    "        dW, db = backward_propagation(num_layers, y, W, A, Z)\n",
    "        W, diff_W, b, diff_b = update_params(num_layers, learning_rate, momentum_factor,\n",
    "                                             W, diff_W, dW, b, db, diff_b)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    if plot_cost:\n",
    "        plot_cost_curve(costs)\n",
    "        \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./mnist/train.csv')\n",
    "X, y = data.drop(columns='label').values, data['label'].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Preprocessing</b>:\n",
    "\n",
    "- $X$: Pixel intensity values are firstly normalized to fall between 0 and 1 by dividing to maximum value of $X$.\n",
    "\n",
    "- $y$: One-hot encoding target vector to $N \\times D_f$ matrix, where $N$ is the number of samples and $D_f$ is the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 784) (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing on X \n",
    "X = (X / X.max())\n",
    "\n",
    "# Preprocessing on y\n",
    "y = np.eye(y.max() - y.min() + 1)[y]\n",
    "\n",
    "# Shape of the data after preprocessing\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Split data to train and test set</b> with ratio 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "W, b = simple_mlp(X_train, y_train, num_units=50, momentum_factor=0.0, num_iterations=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
