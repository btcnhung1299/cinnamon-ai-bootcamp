{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Machine Translation with seq2seq + attention.ipynb","provenance":[{"file_id":"1AdJL5Cw-Qs68hrAs3Zhhi9yfMxKx6unI","timestamp":1583507805685},{"file_id":"1jWoGg8LfbgmvUzeE8RojTZtYW2-_SFZJ","timestamp":1583404415234}],"collapsed_sections":["y5XcE86w9uwZ","4tobVUfbTbG0"],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyNzhBqDtaViImhqr+GLXMwY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"-KiBTWcbQ5K3","colab_type":"code","outputId":"244f8ae3-387b-4b87-8bff-56bba20ac5f4","executionInfo":{"status":"ok","timestamp":1583747294048,"user_tz":240,"elapsed":3213,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"source":["import numpy as np\n","import tensorflow as tf\n","tf.enable_eager_execution()\n","from sklearn.model_selection import train_test_split\n","\n","import os\n","import re\n","import time\n","import html\n","import nltk\n","import string"],"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"y5XcE86w9uwZ","colab_type":"text"},"source":["## Data Gathering\n","\n","IWSLT'15 English-Vietnamese data\n","- Train (133K sentence pairs): [train.en] [train.vi]\n","- Test: [tst2012.en] [tst2012.vi] [tst2013.en] [tst2013.vi]\n","- Vocabularies (**top 50K** frequent words): [vocab.en] [vocab.vi]\n","- Dictionary (extracted from alignment data): [dict.en-vi]"]},{"cell_type":"markdown","metadata":{"id":"0BzYJa4pVQUO","colab_type":"text"},"source":["Download dataset"]},{"cell_type":"code","metadata":{"id":"bpclDW47RCex","colab_type":"code","colab":{}},"source":["SITE_URL = 'https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi'\n","!wget -q -r -l1 --no-parent -e robots=off -R \"index.html*\" $SITE_URL\n","\n","DATA_FOLDER = 'nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bo_xzbx35olC","colab_type":"text"},"source":["Load English and Vietnames vocabularies"]},{"cell_type":"code","metadata":{"id":"z04Yd5uj5S88","colab_type":"code","outputId":"0c6dd2d3-dd15-4ae7-d90f-96d5ca1115dc","executionInfo":{"status":"ok","timestamp":1583747309338,"user_tz":240,"elapsed":18484,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["with open('{}/vocab.vi'.format(DATA_FOLDER), 'rb') as vi_vocab_file:\n","  vi_vocab = vi_vocab_file.read().decode(encoding='utf-8').split('\\n')\n","\n","with open('{}/vocab.en'.format(DATA_FOLDER), 'rb') as en_vocab_file:\n","  en_vocab = en_vocab_file.read().decode(encoding='utf-8').split('\\n')\n","\n","print('Size of Vietnamese vocabulary:', len(vi_vocab))\n","print('Size of English vocabulary:', len(en_vocab))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Size of Vietnamese vocabulary: 7710\n","Size of English vocabulary: 17192\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-9pIuMZwXIEH","colab_type":"text"},"source":["Dataset is organized in parallel order: $i^{th}$ Vietnamese sentence in `train.vi` corresponds to $i^{th}$ English sentence in `train.en`."]},{"cell_type":"code","metadata":{"id":"bFx7_0sHWiqs","colab_type":"code","outputId":"df1caea1-6b86-4395-a04e-c7338df12dae","executionInfo":{"status":"ok","timestamp":1583747309340,"user_tz":240,"elapsed":18474,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["with open('{}/train.vi'.format(DATA_FOLDER), 'rb') as vi_text_file:\n","  vi_text = vi_text_file.read().decode(encoding='utf-8').split('\\n')\n","\n","with open('{}/train.en'.format(DATA_FOLDER), 'rb') as en_text_file:\n","  en_text = en_text_file.read().decode(encoding='utf-8').split('\\n')\n","\n","print('+ Vietnamese text sample:', *vi_text[:2], sep='\\n', end='\\n\\n')\n","print('+ English text sample:', *en_text[:2], sep='\\n')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["+ Vietnamese text sample:\n","Khoa học đằng sau một tiêu đề về khí hậu\n","Trong 4 phút , chuyên gia hoá học khí quyển Rachel Pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .\n","\n","+ English text sample:\n","Rachel Pike : The science behind a climate headline\n","In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1RY_l5Jy90EX","colab_type":"text"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"MzQgxbzfShgA","colab_type":"text"},"source":["### Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"MVEDXGcZjr-M","colab_type":"text"},"source":["Preprocess the text:\n","1. Normalize HTML marks\n","2. Convert all characters to lowercase\n","3. Replace any characters except alphabets in each word with space\n","4. Add start token and end token (if necessary)"]},{"cell_type":"code","metadata":{"id":"bT4sg4ZybPbP","colab_type":"code","outputId":"e09fa096-c202-4c67-e9c0-0068904b3f5c","executionInfo":{"status":"ok","timestamp":1583747316313,"user_tz":240,"elapsed":25434,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["def remove_special_character(w):\n","  # In case of standalone punctutation\n","  if len(w) == 1 and not w.isalpha() and not w.isdigit():\n","    return w\n","  return ''.join(c for c in w if c.isalpha())\n","\n","def preprocess(s):\n","  norm_s = html.unescape(s.lower())\n","  norm_s = ' '.join(remove_special_character(w) for w in norm_s.split())\n","  return norm_s\n","\n","vi_text = [preprocess(sentence) for sentence in vi_text]\n","en_text = [preprocess(sentence) for sentence in en_text]\n","\n","# First 3 pairs of sentences after preprocessed\n","for i in range(3):\n","  print(vi_text[i], en_text[i], '-' * 50, sep='\\n')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["khoa học đằng sau một tiêu đề về khí hậu\n","rachel pike : the science behind a climate headline\n","--------------------------------------------------\n","trong  phút , chuyên gia hoá học khí quyển rachel pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình  hàng ngàn người đã cống hiến cho dự án này  một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .\n","in  minutes , atmospheric chemist rachel pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team  one of thousands who contributed  taking a risky flight over the rainforest in pursuit of data on a key molecule .\n","--------------------------------------------------\n","tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .\n","i d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n","--------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZHbR97Tu6yjD","colab_type":"text"},"source":["### Tokenizing\n","\n","Let define special tokens:\n","- `<sos>`: start token - appear at the beginning of every sentence\n","- `<eos>`: end token - appear at the end of every sentence\n","- `<unk>`: unknown token - replace words that does not exist in vocabulary"]},{"cell_type":"code","metadata":{"id":"t4UtuXDzKQe5","colab_type":"code","colab":{}},"source":["start_token, end_token, unk_token = '<sos>', '<eos>', '<unk>'\n","special_tokens = (start_token, end_token, unk_token)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QjgalFqaBNm-","colab_type":"text"},"source":["\n","Notice that text in English and Vietnamese does not completely align in title of the talk. Hence, we simply remove author name from strings whose ending character is not a punctuation."]},{"cell_type":"code","metadata":{"id":"6K2fI-767e1k","colab_type":"code","colab":{}},"source":["def remove_author(tokens):\n","  if not (tokens and not tokens[-1] in string.punctuation):\n","    return tokens\n","  try:\n","    tokens = tokens[tokens.index(':') + 1 :]\n","  except ValueError:\n","    pass\n","  return tokens"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLjv_otR_rye","colab_type":"code","colab":{}},"source":["def tokenize(text, has_author=False):\n","  tokens = remove_author(text.split()) if has_author else text.split()\n","  return [start_token] + tokens + [end_token]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9BBTX7L8AsNp","colab_type":"code","colab":{}},"source":["vi_tokens = [tokenize(sentence, has_author=False) for sentence in vi_text]\n","en_tokens = [tokenize(sentence, has_author=True) for sentence in en_text]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4sCQIAzL9Oxr","colab_type":"text"},"source":["Normalize words in vocabularies and add special tokens as new words."]},{"cell_type":"code","metadata":{"id":"zwqBJfyS9j2c","colab_type":"code","outputId":"e0a9df94-d942-426d-9c3b-f330b0fd5fdb","executionInfo":{"status":"ok","timestamp":1583747318087,"user_tz":240,"elapsed":27176,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["vi_vocab = set(preprocess(w) for w in vi_vocab).union(special_tokens)\n","vi_vocab.discard('')\n","vi_vocab = sorted(vi_vocab)\n","\n","en_vocab = set(preprocess(w) for w in en_vocab).union(special_tokens)\n","en_vocab.discard('')\n","en_vocab = sorted(en_vocab)\n","\n","# Later mapping offset 1\n","vi_vocab_size, en_vocab_size = len(vi_vocab) + 1, len(en_vocab) + 1\n","print('Vietnamese vocab size:', vi_vocab_size)\n","print('English vocab size:', en_vocab_size)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Vietnamese vocab size: 6026\n","English vocab size: 15305\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"10RBLBt_9nlY","colab_type":"text"},"source":["\n","Create dictonaries mapping from word to index and idx to word."]},{"cell_type":"code","metadata":{"id":"mGHWOhFMaqww","colab_type":"code","colab":{}},"source":["def create_mapping(vocab):\n","  word2idx = {word : idx + 1 for idx, word in enumerate(vocab)}\n","  idx2word = np.array([None] + vocab)\n","  return word2idx, idx2word\n","\n","vi2idx, idx2vi = create_mapping(vi_vocab)\n","en2idx, idx2en = create_mapping(en_vocab)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-JfYRiN_0U1","colab_type":"text"},"source":["###Vectorizing\n","\n","1. Replace infrequent words by unknown tokens\n","2. Map from words to indices\n","3. Pad vectors to the same length"]},{"cell_type":"markdown","metadata":{"id":"Yb6aMAT3Q_LZ","colab_type":"text"},"source":["#### Mapping"]},{"cell_type":"code","metadata":{"id":"cODNH8D__5Z1","colab_type":"code","outputId":"d76bb96f-25d3-4a53-cbbe-77dc8311bc78","executionInfo":{"status":"ok","timestamp":1583747320623,"user_tz":240,"elapsed":29691,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["vectorize = lambda tokens, word2idx : [word2idx.get(w, word2idx[unk_token]) for w in tokens]\n","vi_int = np.array([vectorize(tok, vi2idx) for tok in vi_tokens])\n","en_int = np.array([vectorize(tok, en2idx) for tok in en_tokens])\n","\n","vi_sample_str, en_sample_str = vi_tokens[0], en_tokens[0]\n","print('+ Vietnamese text and its representation:', vi_sample_str, vectorize(vi_sample_str, vi2idx), sep='\\n', end='\\n\\n')\n","print('+ English text and its representation:', en_sample_str, vectorize(en_sample_str, en2idx), sep='\\n')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["+ Vietnamese text and its representation:\n","['<sos>', 'khoa', 'học', 'đằng', 'sau', 'một', 'tiêu', 'đề', 'về', 'khí', 'hậu', '<eos>']\n","[19, 2310, 2057, 5894, 4304, 3193, 4875, 5906, 5446, 2359, 2040, 18]\n","\n","+ English text and its representation:\n","['<sos>', 'the', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n","[17, 13746, 11988, 1252, 24, 2414, 6243, 16]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jyuobMSY-G1S","colab_type":"text"},"source":["#### Padding\n","\n","Vectors should be of the same length. However, using maximum length as standard length for padding causes memory inefficiency as more than 90% of Vietnamese sentences are less 50 words and 40 for English.\n","\n","Hence, we use length at the 90th percentile as threshold and filter all sentences whose number of words less than it."]},{"cell_type":"code","metadata":{"id":"Zpn2zC0-_s15","colab_type":"code","outputId":"76414a91-e096-4ae9-ae26-b199243d7b85","executionInfo":{"status":"ok","timestamp":1583747320630,"user_tz":240,"elapsed":29679,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["vi_lengths = [len(v) for v in vi_int]\n","en_lengths = [len(v) for v in en_int]\n","\n","# Find theshold\n","vi_length, en_length = int(np.percentile(vi_lengths, 90)), int(np.percentile(en_lengths, 90))\n","print('+ VI: max length is {} - 90% is {}'.format(max(vi_lengths), vi_length))\n","print('+ EN: max length is {} - 90% is {}'.format(max(en_lengths), en_length))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["+ VI: max length is 837 - 90% is 48\n","+ EN: max length is 615 - 90% is 40\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LNAs_jKlBeOd","colab_type":"code","colab":{}},"source":["num_samples = vi_int.shape[0]\n","\n","# Get sentences whose number of words are less than the threshold\n","indices = [i for i in range(num_samples) if len(vi_int[i]) <= vi_length and len(en_int[i]) <= en_length]\n","vi_int, en_int = vi_int[indices], en_int[indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wv3tnvClOSBA","colab_type":"code","outputId":"5fb89cfb-96d0-427f-d2b7-f5108444296c","executionInfo":{"status":"ok","timestamp":1583747321836,"user_tz":240,"elapsed":30861,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# Pad vectors to the same length\n","vi_padded = tf.keras.preprocessing.sequence.pad_sequences(vi_int, maxlen=vi_length, padding='post')\n","en_padded = tf.keras.preprocessing.sequence.pad_sequences(en_int, maxlen=en_length, padding='post')\n","\n","print('Shape of data:')\n","print('+ VI:', vi_padded.shape)\n","print('+ EN:', en_padded.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Shape of data:\n","+ VI: (117993, 48)\n","+ EN: (117993, 40)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0u9Bgh_I5W_4","colab_type":"text"},"source":["### Sample data"]},{"cell_type":"code","metadata":{"id":"prfpk8jj5asX","colab_type":"code","outputId":"ef6af903-bd0d-4c47-98f4-ef242783d758","executionInfo":{"status":"ok","timestamp":1583747321837,"user_tz":240,"elapsed":30842,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["train_in, test_in, train_out, test_out = train_test_split(vi_padded, en_padded, test_size=.7, random_state=101)\n","print('Number of training samples:', len(train_in))\n","print('Number of testing samples:', len(test_in))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Number of training samples: 35397\n","Number of testing samples: 82596\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4tobVUfbTbG0","colab_type":"text"},"source":["## Batchifing"]},{"cell_type":"code","metadata":{"id":"mm5jM0U4TdOm","colab_type":"code","outputId":"b6185c12-1985-4869-926d-fcbe0dba21b2","executionInfo":{"status":"ok","timestamp":1583747323257,"user_tz":240,"elapsed":32245,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":0}},"source":["BATCH_SIZE = 64\n","BUFFER_SIZE = 5000\n","steps_per_epochs = len(train_in) // BATCH_SIZE\n","\n","pair_stream = tf.data.Dataset.from_tensor_slices((train_in, train_out))\n","batch_data = pair_stream.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","batch_data"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<DatasetV1Adapter shapes: ((64, 48), (64, 40)), types: (tf.int32, tf.int32)>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"gBbWI5E9b2m4","colab_type":"code","colab":{}},"source":["sample_input, sample_target = next(iter(batch_data))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VnalEfoyehey","colab_type":"text"},"source":["## Modeling"]},{"cell_type":"code","metadata":{"id":"EbNSB1W4e0cJ","colab_type":"code","colab":{}},"source":["embedding_dim = 256\n","num_units = 1024"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v82MNF_6TYsn","colab_type":"text"},"source":["#### Attention\n","\n","$score(h_t, h_s) = v_a^{T} \\tanh{(W_1 h_t + W_2 h_s)}$\n","\n","Attention weights: $\\alpha_{ts} = \\frac{exp{(score(h_t, h_s))}}{\\sum_{s'=1}^{S} exp{(score(h_t, h_s))}}$\n","\n","Context vector: $c_t = \\sum_{s} \\alpha_{ts} h_s$"]},{"cell_type":"code","metadata":{"id":"36R4GoiXTaw3","colab_type":"code","colab":{}},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, num_units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(num_units)\n","    self.W2 = tf.keras.layers.Dense(num_units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, hidden_state, enc_output):\n","    hidden_t = tf.expand_dims(hidden_state, 1)\n","    score = self.V(tf.nn.tanh(self.W1(hidden_t) + self.W2(enc_output)))\n","    weights = tf.nn.softmax(score, axis=1)\n","    context_vector = tf.reduce_sum(weights * enc_output, axis=1)\n","    return context_vector"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTVGxLAPXpbi","colab_type":"text"},"source":["#### Encoder Decoder"]},{"cell_type":"code","metadata":{"id":"bBS8Y-dXx1oS","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, num_units, batch_size):\n","    super(Encoder, self).__init__()\n","    self.num_units = num_units\n","    self.batch_size = batch_size\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.hidden = tf.keras.layers.GRU(num_units, return_sequences=True,\n","                                      return_state=True, recurrent_initializer='glorot_uniform')\n","\n","  def init_hidden_state(self):\n","    return tf.zeros((self.batch_size, self.num_units))\n","\n","  def call(self, x, hidden_state):\n","    output, hidden_state = self.hidden(self.embedding(x), initial_state=hidden_state)\n","    return output, hidden_state\n","\n","\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, num_units, use_attention=True):\n","    super(Decoder, self).__init__()\n","    self.use_attention = use_attention\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.hidden = tf.keras.layers.GRU(num_units, return_sequences=True,\n","                                      return_state=True, recurrent_initializer='glorot_uniform')\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","    self.attention = BahdanauAttention(num_units)\n","  \n","  def call(self, x, hidden_state, enc_output):\n","    x = self.embedding(x)\n","\n","    if self.use_attention:\n","      context_vector = self.attention(hidden_state, enc_output)\n","      x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","    \n","    output, hidden_state = self.hidden(x, initial_state=hidden_state)\n","    output = self.dense(tf.reshape(output, (-1, output.shape[2])))\n","    return output, hidden_state"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTike1s7X0WG","colab_type":"text"},"source":["#### Testing"]},{"cell_type":"code","metadata":{"id":"PihrKMmz1-Lu","colab_type":"code","outputId":"ad441f44-a90d-44a2-8be9-5d882880310c","executionInfo":{"status":"ok","timestamp":1583747330977,"user_tz":240,"elapsed":39925,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["simple_encoder = Encoder(vi_vocab_size, embedding_dim, num_units, BATCH_SIZE)\n","simple_decoder = Decoder(en_vocab_size, embedding_dim, num_units, use_attention=False)\n","attention_encoder = Encoder(vi_vocab_size, embedding_dim, num_units, BATCH_SIZE)\n","attention_decoder = Decoder(en_vocab_size, embedding_dim, num_units, use_attention=True)\n","\n","sample_output, sample_state = simple_encoder(sample_input, simple_encoder.init_hidden_state())\n","sample_simple_decoder_output, _ = simple_decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                                 sample_state, enc_output=sample_output)\n","sample_attention_decoder_output, _ = attention_decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                                       sample_state, enc_output=sample_output)\n","\n","print('Encoder output shape:', sample_output.shape)\n","print('Hidden state shape:', sample_state.shape)\n","print('Decoder output shape:', sample_simple_decoder_output.shape)\n","print('Decoder with attention output shape:', sample_attention_decoder_output.shape)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Encoder output shape: (64, 48, 1024)\n","Hidden state shape: (64, 1024)\n","Decoder output shape: (64, 15305)\n","Decoder with attention output shape: (64, 15305)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vfJDQw8uGKJO","colab_type":"text"},"source":["### Optimizer and Loss function"]},{"cell_type":"code","metadata":{"id":"qWfVUZ38AEk7","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def loss(target, pred):\n","  spare_loss = loss_obj(target, pred)\n","  mask = tf.cast(tf.math.logical_not(tf.math.equal(target, 0)), dtype=spare_loss.dtype)\n","  return tf.reduce_mean(spare_loss * mask)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p2h8XCa1He6r","colab_type":"text"},"source":["### Configure checkpoints"]},{"cell_type":"code","metadata":{"id":"JfRfUvO1BIwO","colab_type":"code","colab":{}},"source":["def config_checkpoints(encoder, decoder, prefix):\n","  decode_name = '{}_decoder'.format(prefix)\n","  checkpoint_dir = './{}_training_checkpoints'.format(prefix)\n","  checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n","  checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder_name=decoder)\n","  return checkpoint_dir, checkpoint_prefix, checkpoint"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I28-GoBIHiMh","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"1j4rdvQtHmo8","colab_type":"code","colab":{}},"source":["def batch_train(target_start_token, input, target, enc_hidden, encoder, decoder):\n","  total_loss = 0\n","  num_words = int(target.shape[1])\n","\n","  with tf.GradientTape() as tape:\n","    # Encoder Decoder\n","    enc_output, enc_hidden = encoder(input, enc_hidden)\n","    dec_input = tf.expand_dims([target_start_token] * BATCH_SIZE, 1)\n","    dec_hidden = enc_hidden\n","    \n","    # Predict next words of i-th word, except start token\n","    for i in range(1, num_words):\n","      pred, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n","      total_loss += loss(target[:, i], pred)\n","      dec_input = tf.expand_dims(target[:, i], 1)     # teacher forcing\n","  \n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  optimizer.apply_gradients(zip(tape.gradient(total_loss, variables), variables))\n","  batch_loss = total_loss / num_words\n","  return batch_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7V2abfhQBT46","colab_type":"code","colab":{}},"source":["def train(sep_token, encoder, decoder, epochs, checkpoint, checkpoint_prefix):\n","  for epoch in range(epochs):\n","    start = time.time()\n","    enc_hidden = encoder.init_hidden_state()\n","    epoch_loss = 0\n","    \n","    for (batch, (input, target)) in enumerate(batch_data.take(steps_per_epochs)):\n","      batch_loss = batch_train(sep_token, input, target, enc_hidden, encoder, decoder)\n","      epoch_loss += batch_loss\n","      if batch % 200 == 0:\n","        print('Epoch {}, batch {}: Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n","    \n","    if (epoch + 1) % 5 == 0:\n","      checkpoint.save(file_prefix=checkpoint_prefix)\n","    print('[Epoch {}] Time: {:.2f}, Loss: {:.4f}'.format(epoch + 1, time.time() - start,\n","                                                        epoch_loss / steps_per_epochs))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PH8RTsWXXWp3","colab_type":"text"},"source":["### Without Attention"]},{"cell_type":"code","metadata":{"id":"ejHfPAhsXZSb","colab_type":"code","outputId":"ca9f05a9-cc65-4db1-d215-5a81875ada9d","executionInfo":{"status":"ok","timestamp":1583514521397,"user_tz":300,"elapsed":6404506,"user":{"displayName":"Nhung Bui","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GieoIegiDFCUmdZLj-F9sD6IKbd-f-hSmDYACYm3w=s64","userId":"04140943803512381438"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["cp_dir, cp_prefix, cp = config_checkpoints(simple_encoder, simple_decoder, 'simple')\n","train(en2idx[start_token], simple_encoder, simple_decoder,\n","      epochs=10, checkpoint=cp, checkpoint_prefix=cp_prefix)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Epoch 1, batch 0: Loss 3.9561\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dWUyC-rReeal","colab_type":"code","colab":{}},"source":["# Restore checkpoint\n","cp.restore(tf.train.latest_checkpoint(cp_dir))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P6ZOmUq-ZKBy","colab_type":"text"},"source":["### With Attention"]},{"cell_type":"code","metadata":{"id":"nUM7OLaBZLjW","colab_type":"code","colab":{}},"source":["attcp_dir, attcp_prefix, attcp = config_checkpoints(attention_encoder, attention_decoder, 'attention')\n","train(en2idx[start_token], attention_encoder, attention_decoder,\n","      epochs=10, checkpoint=attcp, checkpoint_prefix=attcp_prefix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a6itYo9lJYvg","colab_type":"code","colab":{}},"source":["# Restore checkpoint\n","attcp.restore(tf.train.latest_checkpoint(attcp_dir))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-mJo6eO3BGp","colab_type":"text"},"source":["## Translation"]},{"cell_type":"code","metadata":{"id":"orqVra0Me4z0","colab_type":"code","colab":{}},"source":["def translate(sentence, encoder, decoder, max_length=15):\n","  \n","  # Vectorize the sentence and pad to the standard length\n","  s_int = vectorize(tokenize(preprocess(sentence)), vi2idx)\n","  s_padded = tf.keras.preprocessing.sequence.pad_sequences([s_int], maxlen=vi_length,\n","                                                           padding='post')\n","  \n","  trans = []\n","  enc_input = tf.convert_to_tensor(s_padded)\n","  \n","  # Feed input and initial zeros states to encoder\n","  enc_out, dec_hidden = encoder(enc_input, [tf.zeros((1, num_units))])\n","  dec_input = tf.expand_dims([en2idx[start_token]], 0)\n","  \n","  for i in range(max_length):\n","    pred, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n","    next_word_id = tf.argmax(pred[0]).numpy()\n","    next_word = idx2en[next_word_id]\n","    \n","    if next_word == end_token:\n","      break\n","    trans.append(idx2en[next_word_id])\n","    dec_input = tf.expand_dims([next_word_id], 0)\n","    \n","  trans_sentence = ' '.join(trans)\n","\n","  return trans_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RgOMjzTTJK3F","colab_type":"code","colab":{}},"source":["print(translate(u'ngày mai trời sẽ mưa .', simple_encoder, simple_decoder))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RDnyndbfbkaB","colab":{}},"source":["print(translate(u'ngày mai trời sẽ mưa .', attention_encoder, attention_decoder))"],"execution_count":0,"outputs":[]}]}