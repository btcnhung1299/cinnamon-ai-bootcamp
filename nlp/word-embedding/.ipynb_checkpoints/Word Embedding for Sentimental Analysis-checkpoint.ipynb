{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Untitled-checkpoint.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdAGaNa_wpy4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ5c0uhlxMY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0a2660e4-855d-455c-a7ab-8eb3d961b096"
      },
      "source": [
        "# Download WordNet and stopwords\n",
        "nltk.download('wordnet');\n",
        "nltk.download('stopwords');\n",
        "\n",
        "# Download data\n",
        "DATA_LINK = \"http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\"\n",
        "FILE_NAME = 'cornell_movie_dialogs_corpus.zip'\n",
        "!test -f $FILE_NAME || (wget -q $DATA_LINK && unzip -q $FILE_NAME && rm -rf $FILE_NAME)\n",
        "\n",
        "# Read data\n",
        "data_folder = \"./cornell movie-dialogs corpus\"\n",
        "with open('{}/movie_lines.txt'.format(data_folder), 'rb') as movie_lines_file:\n",
        "    lines_data = movie_lines_file.read().decode(encoding='utf-8', errors='ignore')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4L14tClwpzK",
        "colab_type": "text"
      },
      "source": [
        "### Data Gathering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1xG98XJwpzN",
        "colab_type": "text"
      },
      "source": [
        "<b>1.</b> First, we split text by endline symbol (`\\n`) to get list of utterances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFfzfG8BwpzO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a542b742-c2f9-4b6a-f069-ebd69229e5ab"
      },
      "source": [
        "utterances = lines_data.split(sep='\\n')\n",
        "print('Last line (\"{}\") is an empty string, so we remove it from list of utterances.'.format(utterances.pop()))\n",
        "print('Number of utterances:', len(utterances))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last line (\"\") is an empty string, so we remove it from list of utterances.\n",
            "Number of utterances: 304713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcjov0rUwpzS",
        "colab_type": "text"
      },
      "source": [
        "<b>2.</b> Observations on first 10 samples suggest the string ` +++$+++ ` acts as seperator between 5 components of an utterance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCSerKNYwpzT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "ff2b2645-37aa-4dab-9ced-8339b8dc7266"
      },
      "source": [
        "print('> First 10 samples:')\n",
        "print(*utterances[:10], sep='\\n')\n",
        "\n",
        "print()\n",
        "\n",
        "print('> Is there a line which does not have exactly 4 \"+++$+++\"?')\n",
        "print('- {}'.format(any([line.count(' +++$+++ ') != 4 for line in utterances])))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> First 10 samples:\n",
            "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
            "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
            "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
            "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
            "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
            "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
            "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
            "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n",
            "L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
            "L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\n",
            "\n",
            "> Is there a line which does not have exactly 4 \"+++$+++\"?\n",
            "- False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gueuX-GKwpzX",
        "colab_type": "text"
      },
      "source": [
        "<b>3.</b> We then split each utterance by the seperator and convert the whole list to dataframe. Each column in dataframe is renamed regarding its meanings provided in `README.txt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pmoJV7ywpzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "utterances_data = pd.DataFrame([line.split(' +++$+++ ') for line in utterances],\n",
        "                               columns=['lineID', 'characterID', 'movieID', 'chacterter_name', 'text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXzQmirAwpzf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ff176b16-a177-42b3-86b4-58afc2aa887d"
      },
      "source": [
        "print('Number of characters:', utterances_data['characterID'].nunique())\n",
        "print('Number of movies:', utterances_data['movieID'].nunique())\n",
        "print()\n",
        "utterances_data.sample(5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of characters: 9035\n",
            "Number of movies: 617\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lineID</th>\n",
              "      <th>characterID</th>\n",
              "      <th>movieID</th>\n",
              "      <th>chacterter_name</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>192570</th>\n",
              "      <td>L249908</td>\n",
              "      <td>u5621</td>\n",
              "      <td>m373</td>\n",
              "      <td>WILL</td>\n",
              "      <td>We'll do that.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22783</th>\n",
              "      <td>L141236</td>\n",
              "      <td>u722</td>\n",
              "      <td>m45</td>\n",
              "      <td>SALESGIRL</td>\n",
              "      <td>That's no problem. We have those in stock.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101266</th>\n",
              "      <td>L537949</td>\n",
              "      <td>u3020</td>\n",
              "      <td>m197</td>\n",
              "      <td>SPOCK</td>\n",
              "      <td>You must have faith.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29214</th>\n",
              "      <td>L206559</td>\n",
              "      <td>u910</td>\n",
              "      <td>m59</td>\n",
              "      <td>LINDA</td>\n",
              "      <td>I hear some surfer pulled a knife on Mr. Hand ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287259</th>\n",
              "      <td>L618420</td>\n",
              "      <td>u8500</td>\n",
              "      <td>m577</td>\n",
              "      <td>BOND</td>\n",
              "      <td>Trust me.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         lineID  ...                                               text\n",
              "192570  L249908  ...                                     We'll do that.\n",
              "22783   L141236  ...         That's no problem. We have those in stock.\n",
              "101266  L537949  ...                               You must have faith.\n",
              "29214   L206559  ...  I hear some surfer pulled a knife on Mr. Hand ...\n",
              "287259  L618420  ...                                          Trust me.\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PnRYDVgwpzj",
        "colab_type": "text"
      },
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuQPOMZlwpzk",
        "colab_type": "text"
      },
      "source": [
        "<b>1.</b> We define normal characters consisting of alphabetic letters and basic sentence punctuations. Others are considered as special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmVL8oRgwpzl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eefa9717-f754-424a-bb3d-b7a41328e04c"
      },
      "source": [
        "characters = set()\n",
        "utterances_data['text'].apply(lambda text : characters.update(list(text)));\n",
        "\n",
        "# Set of special characters is the intersection\n",
        "# between set of letters appeared in the dataset and normal characters\n",
        "special_characters = characters.difference(string.ascii_letters + '.,!? \\'')    \n",
        "\n",
        "print('List of {} special characters:'.format(len(special_characters)))\n",
        "print(*special_characters, sep=', ')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of 35 special characters:\n",
            "_, &, 9, 7, $, ^, |, `, %, <, 4, 6, #, +, ), 3, [, -, ~, }, =, *, 0, 5, :, >, \t, ;, 1, \", {, 2, ], /, 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3-7zBpCwpzp",
        "colab_type": "text"
      },
      "source": [
        "<b>2.</b> Remove special characters from text of utterances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1lWTPh1wpzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4c6d52d0-e1ff-407d-edaf-b401967a7969"
      },
      "source": [
        "preprocess = lambda text : ''.join([c for c in text if c not in special_characters])\n",
        "sample_text = utterances_data['text'].sample().values[0]\n",
        "\n",
        "print('Sample text:', sample_text)\n",
        "print('After preprocessed:', preprocess(sample_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text: -- understand what you're asking for here. The Massey pre-nup provides that in the event of a dissolution of the marriage for any reason, both parties shall leave it with whatever they brought in, and earned during. No one can profit from the marriage. The pre-nup protects the wealthier party.\n",
            "After preprocessed:  understand what you're asking for here. The Massey prenup provides that in the event of a dissolution of the marriage for any reason, both parties shall leave it with whatever they brought in, and earned during. No one can profit from the marriage. The prenup protects the wealthier party.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o90l9fERwpzu",
        "colab_type": "text"
      },
      "source": [
        "<b>3.</b> Lower casing, lemmatization and repeat removing are utilized to normalize utterance tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AILAJ_C8wpzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f679f2ca-6d26-4cc7-e243-44e51943d4a6"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Create a simple RepeatReplacer using regex\n",
        "class RepeatReplacer():\n",
        "    def __init__(self):\n",
        "        self.pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
        "        self.repl = r'\\1\\2\\3'\n",
        "    \n",
        "    def replace(self, word):\n",
        "        if wordnet.synsets(word):\n",
        "            return word\n",
        "        repl_word = self.pattern.sub(self.repl, word)\n",
        "        \n",
        "        # Recursively replace until it can't be replaced anymore\n",
        "        return repl_word if repl_word == word else self.replace(repl_word)\n",
        "\n",
        "# Testing RepeatReplacer object\n",
        "RepeatReplacer().replace('aaaaaaaaaaaaaaarghhhh')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'argh'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmQt9Vq2wpzy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e99ab56-d267-4c0a-c3ef-7c4b6a67468f"
      },
      "source": [
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "\n",
        "replacer = RepeatReplacer()\n",
        "stemmer = SnowballStemmer(language='english')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "tokenize = lambda text : [lemmatizer.lemmatize(stemmer.stem(replacer.replace(\n",
        "                          word.strip(string.punctuation))))\n",
        "                          for word in text.split()]\n",
        "\n",
        "sample_text = utterances_data['text'].sample().values[0]\n",
        "print('Sample text:', sample_text)\n",
        "print('After tokenized:', tokenize(sample_text))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample text: Is this one art deco or art nouveau?\n",
            "After tokenized: ['is', 'this', 'one', 'art', 'deco', 'or', 'art', 'nouveau']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZtHKaIOwp0B",
        "colab_type": "text"
      },
      "source": [
        "### Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn_ZHda6AcG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3R7tJWkUwp0D",
        "colab_type": "text"
      },
      "source": [
        "#### Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42KtjSKfwp0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e71e9b10-27b7-4b2f-ced2-d20e97ef5102"
      },
      "source": [
        "# Naive BoW\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vectorizer = CountVectorizer(preprocessor=preprocess,\n",
        "                                   tokenizer=tokenize,\n",
        "                                   stop_words=stop_words)\n",
        "bow_vec = count_vectorizer.fit_transform(utterances_data['text'].values)\n",
        "\n",
        "print('Shape:', bow_vec.shape)\n",
        "print(*bow_vec[0].toarray())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'l', 'nedn', \"nedn't\", 'onc', 'onli', 'ourselv', \"should'v\", \"that'l\", 'themselv', 'veri', 'wa', 'whi', \"you'l\", \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape: (304713, 47101)\n",
            "[0 0 0 ... 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3TPKUj9wp0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "7dfb403b-b472-4a87-dcdd-3e5c940d63db"
      },
      "source": [
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(preprocessor=preprocess,\n",
        "                                   tokenizer=tokenize,\n",
        "                                   stop_words=stop_words)\n",
        "tfidf_vec = tfidf_vectorizer.fit_transform(utterances_data['text'].values)\n",
        "\n",
        "print('Shape:', tfidf_vec.shape)\n",
        "print(*tfidf_vec[0].toarray())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'l', 'nedn', \"nedn't\", 'onc', 'onli', 'ourselv', \"should'v\", \"that'l\", 'themselv', 'veri', 'wa', 'whi', \"you'l\", \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Shape: (304713, 47101)\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYWQnBrmwp0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}